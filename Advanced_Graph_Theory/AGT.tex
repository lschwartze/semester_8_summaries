\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % definition numbers are dependent on theorem numbers
\theoremstyle{lemma}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{corollary}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{example}
\newtheorem{example}[theorem]{Example}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{AGT Summary}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\newpage
	\section{Netzwerke und Zentralität}
	\subsection{Charakterisierung der wichtigsten Ecke}
	Hierfür gibt es mehrere Möglichkeiten
	\begin{itemize}
		\item größter Einfluss
		\item wichtig für Informationsfluss
	\end{itemize}
	Die Wichtigkeit wird mit einem \underline{Zentralitätsmaß} gemessen.
	\begin{definition}
		Zentralitätsmaße sind sehr unterschiedlich. Es muss nur erfüllt sein, dass bei einem Sterngraphen das Zentrum das größte Zentralitätsmaß erhält. Möglich sind Bewertungen nach
		\begin{enumerate}
			\item dem Maximalgrad (\textit{degree centrality})
			\item der durchschnittlichen Entfernung zu anderen Ecken (\textit{closeness centrality}) (bzw. der Kehrwert davon)
			\item der Anzahl der Komponenten, die mit dieser Ecke verbunden sind (\textit{betweenness centrality}). Dafür sei $\sigma_{s,t}$ die Anzahl der kürzesten $s-t$-Wege. $\sigma_{s,t}(v)$ für $v\neq s,t$ ist dann die Anzahl der kürzesten $s-t$-Wege, die durch $v$ gehen. Damit gilt \[betweenness(v) = \sum_{s,t \in V()G\setminus\{v\}} \frac{\sigma_{s,t}(v)}{\sigma_{s,t}}\]
		\end{enumerate}
	\end{definition}
	\subsection{Berechnung der Zentralitätsmaße}
	Wir führen nur die Berechnung der betweenness ein. Die anderen beide Maße sind sehr einfach.\\
	Der Algorithmus zur Berechnung von $\sigma_{s,t}$ ist an Dijkstra angelehnt. Beginnend mit $s$ wird die Anzahl der Nachbarn von $s$ bestimmt. Anschließend die Anzahl der Knoten mit Abstand 2 usw.
	Um die Komplexität der Algorithmen zu bestimmen, werden im Folgenden einige Annahmen getroffen:
	\begin{enumerate}
		\item Kotenadjazenz kann in $\mathcal{O}(1)$ bestimmt werden
		\item Kanteninzidenz kann in $\mathcal{O}(1)$ bestimmt werden
		\item die Nachbarschaft eines Knoten wird in $\mathcal{O}(1)$ pro Knoten bestimmt
		\item die zu einem Knoten inzidenten Kanten können in $\mathcal{O}(1)$ pro Kante bestimmt werden
		\item alle elementaren Operationen (z.B. Kante löschen) in $\mathcal{O}(1)$.
	\end{enumerate}
	Auf diese Weise kann man leicht sehen, dass die Laufzeit zur Berechnung von $\sigma_{s,t}$ für alle $s,t$ in $\mathcal{O}(n\cdot m)$ implementiert werden kann.
	Wir nehmen nun an, $\sigma_{s,t}$ sei bekannt und wir definieren \[\rho_s(v) = \sum_{t \neq v} \frac{\sigma_{s,t}(v)}{\sigma_{s,t}}\] Kennt man nun alle $\rho_s(v)$, dann ist \[betweenness(v) = \frac{1}{2} \sum_{s \neq v} \rho_s(v)\]
	\begin{lemma}
		Sei $v$ ein Knoten mit Distanz mindestens $d \geq 1$ zu $s$ und sei $L$ die Menge der Knoten mit Distanz $d+1$ zu $s$. Dann ist \[\rho_s(v) = \sum_{w \in L\cap N(v)} \frac{\sigma_{s,v}}{\sigma_{s,w}}(1+\rho_s(w)\]
	\end{lemma}
	Mit dieser Überlegung lässt sich ein Algorithmus finden, der die \textit{betweenness} jedes Knotens in $\mathcal{O}(mn)$ berechnet.
	\subsection{Random Walks auf Graphen}
	Wir wählen zunächst einen Startknoten $v_0$ bezüglich einer Wahrscheinlichkeitsverteilung $\pi^{(0)}$. Anschließend wird mit Gleichverteilung ein zufälliger Nachbar $v_1$ von $v_0$ gezogen usw. Wenn man sich nun die Frage stellt, was die Wahrscheinlichkeit ist, dass der erste gezogenen Knoten (d.h. $v_1$) gleich dem Knoten $u$ ist, dann entspricht das der Wahrscheinlichkeit, dass $u$ ein Nachbar von $v_0$ ist mal der Wahrscheinlichkeit, dass anschließend $u$ gezogen wird. Da der zweite Schritt gleichverteilt ist, ergibt sich \[\pi_u(1) = \sum_{v \in N(u)} \pi_v^{(0)} \cdot \frac{1}{d(v)}\] Das wird geschrieben als Transition Matrix mit \[T_{uv} = \begin{cases}
		\frac{1}{d(v)}, & \text{ wenn } uv \in E\\
		0, & \text{ sonst}
	\end{cases}\]
	Schreibt man die Wahrscheinlichkeitsverteilung $\pi^{(0)}$ einfach als Vektor, dessen Komponenten zu 1 addieren, ergibt sich \[\pi^{(n+1)} = T\pi^{(n)}\] für $n\geq 0$. Um zu überprüfen, ob ein bestimmter Knoten im Random Walk jemals besucht wird, muss die Grenzwertverteilung bestimmt werden \[\pi^* = \lim_{k \to \infty} T^k\pi^{(0)}\]
	Existiert $\pi^*$, dann ist $\pi^{k}$ eine Cauchy-Folge und man kann leicht sehen, dass $T\pi^* = \pi^*$. Dann ist $\pi^*$ also ein Eigenvektor zum Eigenwert 1 von $T$.
	\begin{theorem}[Perron-Frobenius]
		\label{thm: PF}
		Sei $A \in \mathbb{R}^{n \times n}$ sodass $\exists k \in \mathbb{N}$ mit $A_{ij}^k > 0$ für alle $i,j \in [n]$. Dann gibt es einen eindeutigen Eigenwert $\lambda^*$ mit größtem Betrag. Wenn $\lambda^* >0$ gibt es einen positiven Eigenvektor $v^*$ zu $\lambda^*$ und alle anderen Eigenvektoren zu $\lambda^*$ sind Vielfache von $v^*$. Ist außerdem $\lambda^* = 1$, dann konvergiert $v^{(k+1)} = Av^{(k)}$ gegen ein Vielfaches von $v^*$ für alle positiven Startvektoren $v^{(0)} > 0$.
	\end{theorem}
	Damit kann man sich überzeugen, dass $T$ Eigenwert 1 hat und dass das der größte Eigenwert ist. Außerdem erfüllt $T$ die Eigenschaft aus obigem Theorem, wenn $G$ zusammenhängend und nicht bipartit ist.
	\subsection{Eigenwert Zentralität}
	Für einen Knoten $v$ verwenden wir wieder die Matrix $T$ und nehmen $\pi^* > 0$ als den Eigenvektor zum Eigenwert 1 mit $||\pi^*|| = 1$. Der Eintrag $\pi^*_v$ ist dann die Eigenwert Zentralität von $v$.\\
	Das Problem dieses Zentralitätsbegriffs ist, dass man den Eigenwert recht leicht erraten kann. Betrachte dazu \[\overline{\pi}_v = \frac{d(v)}{2\left|E\right|} \; \forall v \in V\]
	Es ist leicht zu sehen, dass dieser Vektor ein Eigenvektor zum Eigenwert 1 ist. Es folgt also, dass wir einen neuen Begriff haben, der aber sehr ähnlich zur \textit{degree centrality} ist. In gerichteten Graphen ist der Begriff ein wenig hilfreicher. Der wichtigste Unterschied ist die modifizierte Matrix $T$ mit \[T_{vu} = \begin{cases}
		\frac{1}{d^+(u)}, & \text{wenn es eine Kante von $u$ nach $v$ gibt}\\
		0, & \text{ sonst}
	\end{cases}\]
	Das größere Problem sind Senken (d.h. Knoten $v$ mit ausgehendem Grad $d^+(v) = 0$). Das kann gelöst werden, indem der Prozess neugestartet wird (d.h. eine Kante zu jedem anderen Knoten eingeführt wird).
	\subsection{PageRank}
	PageRank ist der Suchalgorithmus von Google. Er funktioniert in den folgenden Schritte, die sehr ähnlich zum Eigenwertzentralität sind \begin{enumerate}
		\item Wähle unter Gleichverteilung einen Startknoten
		\item Mit Wahrscheinlichkeit $1-\alpha$ ($\alpha$ konstant) wähle einen Nachbarn und gehe dorthin.
		\item Mit Wahrscheinlichkeit $\alpha$ wähle einen neuen Startknoten.
	\end{enumerate}
	Die Transitionsmatrix definiert nun eine andere Matrix \[P = (1-\alpha)T + \frac{\alpha}{n} J\] wobei $J$ die Matrix mit nur 1 Einträgen ist. Es ergibt sich der Prozess \[\pi^{(k+1)} = P\pi^{(k)}\] Da $P$ positiv ist, ergibt Theorem \ref{thm: PF} die Existenz der Grenzwertverteilung $p_v^*$. Es gilt PageRank($v$) = $\pi^*_v$. Da der erste Teil von $P$ \textit{sparse} ist, kann die Iteration relativ effizient durchgeführt werden.
	\section{Clustering}
	Wie führen zunächst den Begriff des Clustering-Koeffizienten ein. Sei $v \in V$. dann ist \[C(v) = \frac{\left|E_G[N(v)]\right|}{\binom{\left|N(v)\right|}{2}}\] Der durchschnittliche Clustering-Koeffizient ist dann \[C(G) = \frac{1}{\left|V\right|} \sum_{v \in V} C(v)\] Ein Zufallsgraph mit Kantenwahrscheinlichkeit $p$ hat im Erwartungswert eine Kantendichte $\frac{\left|E\right|}{\binom{n}{2}}$ von ungefähr $p$. Der Clustering-Koeffizient ist ebenso ungefähr $p$.
	\subsection{Berechnung des Clustering Koeffizienten}
	Es ist leicht zu sehen, dass man den Clustering Koeffizienten eines einzelnen Knotens $v$ in $\mathcal{O}(d(v)^2)$ berechnen kann. Um den durchschnittlichen Wert zu bestimmen genügt daher eine Laufzeit von $\mathcal{O}(\sum_{v \in V(G)} d(v)^2)$. Die Summe lässt sich nach oben abschätzen durch $2mn$ wodurch die Laufzeit bei $\mathcal{O}(2mn)$ liegt.\\
	Ist $d(v)$ klein, so ist der Algorithmus sehr effizient, aber ist $d(v) >> \sqrt{m}$ so lässt sich eine Verbesserung erzielen, indem für jede Kante $uw$ überprüft wird, ob $u,v,w$ ein Dreieck bilden. Kombiniert man diese beiden Überlegungen zu einem Algorithmus mittels einer Fallunterscheidung, so erhält man einen Algorithmus zur Berechnung des durchschnittlichen Clustering Koeffizienten mit einer Laufzeit von $\mathcal{O}(m^\frac{3}{2})$.\\
	Es gibt außerdem einen randomisierten Ansatz für die Schätzung des durchschnittlichen Clustering Koeffizienten auf Graphen mit Minimalgrad mindestens 2. Hierfür wird zunächst eine Konstante $k\in\mathbb{N}$ festgelegt. Anschließend werden nacheinander $k$ Knoten $v_1,...,v_k$ zufällig gezogen und aus $N(v_i)$ werden jeweils zwei Nachbarn $u_i,w_i$ zufällig gezogen. Es wird gezählt, wie viele dieser Nachbarn der $k$ Knoten mit $v_i$ ein Dreieck aufspannen und diese Anzahl anschließend durch $k$ geteilt.
	\begin{theorem}
		Sei $\varepsilon>0, \delta>0$ und $k=\lceil\ln\left(\frac{2}{\delta}\right)/(2\varepsilon^2)\rceil$. Dann hat der Algorithmus eine Laufzeit von $\mathcal{O}(\ln\left(\frac{1}{\delta}\right)/\varepsilon^2\cdot \ln n)$ und mit Wahrscheinlichkeit mindestens $1-\delta$ unterscheidet sich der berechnete Wert um maximal $\varepsilon$ vom tatsächlichen Wert.
	\end{theorem}
	Sei $G$ ein Graph mit $V(G) = U \dot\cup W$. Wir schreiben \[\partial_G = \{uw \in E(G): u \in U, w \in W\}\] Für $A \subset V$ ist $\partial_G(A)$ die Anzahl der Kanten zwischen $A$ und $v\setminus A := B$. Ist $w$ eine Funktion, die jeder Kante ein Gewicht zuordnet, definiere \[w(F) = \sum_{e \in F} w(e)\] für alle $F\subset E$.
	\begin{definition}
		Die \textit{expansion} von $A$ ist \[\frac{w(\partial_G(A))}{\min\{\left|A\right|,\left|B\right|\}}\]
		Der \textit{ratio cut} von $A$ ist \[\frac{w(\partial_G(A))}{\left|A\right|\left|B\right|}\]
	\end{definition}
	\subsection{Gomory-Hu Clustering}
	\begin{definition}
		Sei $G$ ein Graph und $w$ die Kantengewichte. Der Gomory-Hu-Baum $T$ für $G$ ist ein Graph mit \begin{itemize}
			\item $V(T) = V(G)$
			\item beim Löschen einer Kante $uv$ aus dem Baum entstehen zwei Komponenten $T_{uv}(u)$ und $T_{uv}(v)$. Für alle $uv \in E(T)$ soll gelten \[w(\partial_G(T_{uv}(u))) = \min_{X\subseteq V(G): v\notin X \ni u} w(\partial_G(X))\] 
		\end{itemize}
		Wir definieren darauf aufbauend \[\lambda(s,t) = \min_{X\subseteq V(G): t \notin X \ni s} w(\partial_G(X))\]
	\end{definition}
	\begin{lemma}
		Sei $G$ ein Graph mit Kantengewichten $w$ und $T$ ein Gomory-Hu-Baum für $G,w$. Seien $s,t \in V(G), s\neq t$, sei $P$ der $st$-Weg in $T$ und $uv \in E(P)$ mit \[\min_{ab \in E(P)} w(\partial_G(T_{ab}(a))) = w(\partial_G(T_{uv}(u)))\] Dann ist $w(\partial_GT_{uv}(u)) = \lambda(s,t)$.
	\end{lemma}
	\begin{theorem}
		Für alle $G,w$ existiert ein Gomory-Hu-Baum. Ein solcher Baum kann in $\mathcal{O}(n\tau)$ berechnet werden. $\tau$ ist dabei die Laufzeit um einen gewichtsminimalen $s-t$ Schnitt für beliebige $s,t$ zu finden.
	\end{theorem}
	Mit diesem Konzept kann nun ein Clustering in den folgenden Schritten gefunden werden \begin{enumerate}
		\item füge einen universellen Knoten $t$ mit Kantengewichten $\alpha$ zurück
		\item berechne den G-H-Baum $T$
		\item gib die Komponenten von $T-t$ als Cluster zurück
	\end{enumerate}
	\begin{lemma}
		Wir arbeiten auf einem Graphen $G$ mit Gewichten $w$. Mit dem G-H-clustering erreichen wir ein Cluster $C \subseteq V(G)$. Dann gilt \[\frac{w(\partial_GC)}{\left|V\setminus C\right|} \leq \alpha\]
	\end{lemma}
	\begin{lemma}
		Teilt man in der Situation von oben das Cluster $C$ noch weiter in $Q,P$, dann gilt \[\frac{w(\partial_G(P,Q))}{\min\{\left|P\right|,\left|Q\right|\}}\]
	\end{lemma}
	\subsection{Berechnung des Gomory-Hu Baums}
	\begin{lemma}
		Die Gewichte eines Cuts sind submodular. Für alle $U,W\subseteq V$ gilt \[w(\partial U) + w(\partial W) \geq w(\partial (W\cap U)) + w(\partial (U \cup W))\]
	\end{lemma}
	\begin{lemma}
		Seien $s,t \in V(G)$ und sei für $X\subseteq V$ $\partial_G X$ ein minimaler $s-t$-Cut. Nun wird $X$ zu einem neuen Knoten $v_x$ kontrahiert. Wobei mehrfache Kanten als eine Kante mit der Summe der Gewichte eingeführt wird. Sei $p,q \in V\setminus X$ und $U\subseteq V\setminus X$ sodass $\partial_{G/X}(U\cup v_x)$ ein minimaler $p-q$-Cut om $G/X$ ist. Dann ist $\partial_G(U\cup X)$ ein minimaler $p-q$-Cut in $G$.
	\end{lemma}
	\begin{definition}[Teil GH Baum]
		Sei $R\subseteq V(G)$. Dann ist ein Baum $T = (R,F)$ mit einer Partition $(C_r)_{r\in R}$ von $R$ ein GH Baum für $R$, wenn \[\forall uv \in F: \partial_G \left(\bigcup_{r \in V(T_{uv}(u))} C_r\right)\] Ist $R=V(G)$, dann entspricht der GH-Baum für $R$ dem GH-Baum für $G$.
	\end{definition}
	Mit dieser Überlegung lässt sich der GH-Baum von $G$ rekursiv aufbauen.
	\subsection{ratio cuts}
	Die Idee ist, dass für eine gegebenes $k \in \mathbb{N}$ eine Partition $C_1,...,C_k$ berechnet wird, sodass \[\sum_{i=1}^{k} w(\partial C_i)\] minimal ist. Damit nicht nur isolierte Knoten geclustert werden, soll der \textit{ratio cut} minimiert werden: \[\min_{C_1,...,C_k} \sum_{i=1}^k \frac{w(\partial C_i)}{\left|C_i\right|}\] Da dieses Problem aber NP-schwer ist, soll stattdessen eine Annäherung gefunden werden.
	\begin{definition}
		Wie immer ist $G=(V,E)$ und $w$ eine Gewichtsfunktion auf den Kanten. Die Laplace Matrix $L \in \mathbb{R}^{V\times V}$ von $G$ ist \[L_{u,v} = \begin{cases}
			-w_{uv}, & \text{ if } uv \in E\\
			\sum_{e \in \delta(u)} w(e), & \text{ if } u=v\\
			0, & \text{ sonst}
		\end{cases}\]
		Man kann schreiben $L = B^T\cdot D \cdot B$ wobei $D \in \mathbb{R}^{E \times E}$ im Feld $(e,e)$ das Gewicht $w(e)$ hat und 0 sonst. $B$ ist aus $\mathbb{R}^{E \times V}$. Für $B$ geben wir allen Kanten aus $G$ eine Richtung vor. In der Spalte $e = (u,v)$ steht $1$ in Spalte $v$, $-1$ in Spalte $u$, wenn $(v,u)$ eine Kante ist und 0 sonst.
	\end{definition}
	Wir können nun $D^{1/2}$ definieren als jede Matrix die Wurzel genau die Einträge der Matrix $D$ hat. Diese Definition ist daher sinnvoll, da $D$ eine Diagonalmatrix ist. Es folgt $L = (D^{1/2}B)^T(D^{1/2}B)$ und \[x^T L x = ||D^{1/2}Bx||_2^2 = \sum_{uv \in E} w_{uv}(x_u-x_v)^2\]
	\begin{lemma}
		Seien $G$ und $w$ wie immer. Sei $L$ die Laplace Matrix von $G$. Dann \begin{enumerate}
			\item $L$ ist symmetrisch und positiv semi-definit
			\item kleinster Eigenwert ist 0
			\item ist $\mathcal{C}$ die Menge der Komponenten von $G$. Dann ist $\chi_C, \, C \in \mathcal{C}$ orthogonale Eigenbasis des Eigenwerts 0.
		\end{enumerate}
	\end{lemma}
	Zurück zu ratio cuts. Sei $A \subsetneq V$ mit $A\neq \varnothing$. Wähle \[\mathbb{R}^V \ni z = \sqrt{\frac{\left|\overline{A}\right|}{\left|A\right|}} \chi_A - \sqrt{\frac{\left|A\right|}{\left|\overline{A}\right|}}\chi_{\overline{A}}\] woraus folgt \[z^TLz = \left|V\right|\sum_{uv \in \partial A} w_{uv}(\frac{1}{\left|A\right|} + \frac{1}{\left|\ofsum_{j=1, j\neq i}^k \left|\partial(C_i,C_j)\right|\] In der Theorie ist dieses Verfahren sehr interessant, allerdings normalerweise nicht praktisch umsetzbar. Deswegen wird das nicht weiter verfolgt.
		\subsection{Cluster-Metrik}
		Wenn die verschiedenen Algorithmen miteinander verglichen werden, ist ein Test-Graph nötig. Solche Testfälle werden meist so aufgebaut, dass ein objektiv bestes Clustering existiert (\textit{ground truth}). Dann muss verglichen werden, welcher Algorithmus ein Clustering produziert, dass am nächsten an dieses Clustering heranreicht. Dafür sind Clustering-Metriken nötig.\\
		\underline{Rand Distanz}: Seien $A,B$ zwei Clusterings. Wir nennen $u,v \in V$ eine Unstimmigkeit, wenn $\exists a \in A: u,v \in a$ aber $\not\exists b \in B: u,v \in b$ oder umgekehrt. $d(A,B)$ als die Anzahl der Unstimmigkeiten ist eine Metrik. Diese Metrik ist leicht zu berechnen, denn \[d(A,B) = \sum_{a \in A} \binom{\left|a\right|}{2} + \sum_{b \in B} \binom{\left|b\right|}{2} - 2\sum_{a\in A, b \in B} \binom{\left|a\cap b\right|}{2}\]
		\underline{Entropie}:
		Sei $V$ der Größe $n$ und seien $A,B,C$ drei Clusterings. Diese werden in folgender Weise als Zufallsvariablen modelliert: wähle ein $v\in V$ mit gleichmäßiger Wahrscheinlichkeit. Dann ist \[X_A: V \to A, v \mapsto a \ni v\] eine Zufallsvariable wobei \[\mathbb{P}[X_A = a] = \frac{\left|a\right|}{n}\] Es sei dann weiter \[H(A) = H(X_A) = -\sum_{a \in A} \frac{\left|a\right|}{n} \log_2\left(\frac{\left|a\right|}{n}\right)\] Die Entropie des Clusterings. Weiter ist \[H(A,B) = -\sum_{a\in A, b \in B} \frac{\left|a \cap b\right|}{n} \log_2\left(\frac{\left|a\cap b\right|}{n}\right)\] und $VI(A,B) = 2H(A,B) - H(A) - H(B)$ die \textit{variation of information}. Diese ist dann eine Metrik.
		\section{Streaming}
		\subsection{HyperLogLog}
		Es wird eine Reihe an Zahlen eingelesen, die nicht in den Speicher passt. Wie kann die Anzahl der paarweise verschiedenen Zahlen festgestellt werden?\\
		Die Zahlenreihe $a_1,...a_n$ wird beim Einlesen in Binärdarstellung umgewandelt. Die Binärzahlen werden anschließend mit einer zufälligen aber deterministischen Transformation in eine andere Binärzahl konvertiert. Bei jeder transformierten Zahl werden die letzten Stellen betrachtet und die Anzahl der 0en am Ende gezählt. Die maximale Zahl $R$ von 0en am Ende einer Zahl wird gespeichert. Ausgegeben wird $2^R$.
		\begin{theorem}
			Sei $F_0$ die Anzahl der paarweise verschiedenen Elemente eines Streams von $m$ Zahlen und sei $Y$ der Output des Algorithmus ($Y=2^R$). Jedes $a$ des Streams liegt in $\{1,...,n\}$. Es wird $\mathcal{O}(\log n)$ Platz gebraucht und für alle Integer $c > 2$ ist \[\mathbb{P}\left[\frac{1}{c} \leq \frac{Y}{F_0}\leq c\right] \geq 1-\frac{3}{c}\] 
		\end{theorem}
	\subsection{Streaming mit Graphen}
	Die Knoten des Graphens sind initial im Speicher. Dieser ist nur $\mathcal{O}(n\cdot \log(n)^p)$ für ein $p$. Das heißt die Kanten (ungefähr $n^2$) können nicht gespeichert werden. Der Stream besteht nun aus den Kanten, also $e_1,e_2,...,e_m$.
	\end{document}